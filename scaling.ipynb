{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a480226b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import pandas as pd  \n",
    "import sqlalchemy as sa\n",
    "import acquire\n",
    "import sklearn.preprocessing\n",
    "import wrangle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import env\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb3afb",
   "metadata": {},
   "source": [
    "### 1. Apply the scalers we talked about in this lesson to your data and visualize the results for the unscaled and scaled distribution .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_and_validate, x_test = train_test_split(x, random_state=123)\n",
    "x_train, x_validate = train_test_split(x_train_and_validate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bea5b1",
   "metadata": {},
   "source": [
    "### 2. Apply the .inverse_transform method to your scaled data. Is the resulting dataset the exact same as the original data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d01eec",
   "metadata": {},
   "source": [
    "## Min-Max Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "# Note that we only call .fit with the training data,\n",
    "# but we use .transform to apply the scaling to all the data splits.\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_validate_scaled = scaler.transform(x_validate)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(x_train, bins=25, ec='black')\n",
    "plt.title('Original')\n",
    "plt.subplot(122)\n",
    "plt.hist(x_train_scaled, bins=25, ec='black')\n",
    "plt.title('Scaled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350780d2",
   "metadata": {},
   "source": [
    "## Standard Scaler\n",
    " Standardization is a linear transformation of our data such that is looks like the standard normal distribution. That is, it will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "x\n",
    "′\n",
    "=\n",
    "x\n",
    "−\n",
    "¯\n",
    "x\n",
    "σ\n",
    "x\n",
    "\n",
    " Sometimes this is split into two operations:\n",
    "\n",
    " * scaling is dividing each data point by the standard deviation. This causes the resulting dataset to have a standard deviation of 1.\n",
    "* centering is subtracting the mean from each data point. This causes the resulting dataset to have a mean of 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "# Note that we only call .fit with the training data,\n",
    "# but we use .transform to apply the scaling to all the data splits.\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_validate_scaled = scaler.transform(x_validate)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(x_train, bins=25, ec='black')\n",
    "plt.title('Original')\n",
    "plt.subplot(122)\n",
    "plt.hist(x_train_scaled, bins=25, ec='black')\n",
    "plt.title('Scaled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eeaaa",
   "metadata": {},
   "source": [
    "### 3. Read the documentation for sklearn's QuantileTransformer. Use normal for the output_distribution and apply this scaler to your data. Visualize the result of your data scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a03cc",
   "metadata": {},
   "source": [
    "## RobustScaler\n",
    "\n",
    "A robust scaler is another linear transformation that follows the same idea as the standard scaler but uses parameters that are more robust to outliers.\n",
    "\n",
    "x\n",
    "′\n",
    "=\n",
    "x\n",
    "−\n",
    "med\n",
    "(\n",
    "x\n",
    ")\n",
    "IQR\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f0fbe",
   "metadata": {},
   "source": [
    "### 4. Use the QuantileTransformer, but omit the output_distribution argument. Visualize your results. What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.RobustScaler()\n",
    "# Note that we only call .fit with the training data,\n",
    "# but we use .transform to apply the scaling to all the data splits.\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_validate_scaled = scaler.transform(x_validate)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(x_train, bins=25, ec='black')\n",
    "plt.title('Original')\n",
    "plt.subplot(122)\n",
    "plt.hist(x_train_scaled, bins=25, ec='black')\n",
    "plt.title('Scaled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a5ef5",
   "metadata": {},
   "source": [
    "### 5. Based on the work you've done, choose a scaling method for your dataset. Write a function within your prepare.py that accepts as input the train, validate, and test data splits, and returns the scaled versions of each. Be sure to only learn the parameters for scaling from your training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e07ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
